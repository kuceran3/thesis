multidimensional array data structure a indexing treba 
kmp dat k pattern matchingu na stringach

at se nejmenujou kapitoly stejne jako clanky

ten learning to hash se similarity hashem spojit nebo napsat jak se k sobe poji

v problem specu dat priklady

ze specifikace dat odkazy do textu kde se to dal rozviji

!!
na konci introduction rozepsat do detailu co je cil kazdy kapitoly, odstavec skoro o kazdym
!!

v similarity measures proc jsme tak preskocili do vektoru najednou, na zacatek dopsat proc zase tohle
vsude dopisovat co se tam bude dit

obhajit si domain-dependent sekci simil measures
neco jako ze jsou measures pro vsechny tyhle kategorie a ja se zamerila na tohle

presunout measures na polich az za measures na stringach

binarni format presunout do apendixu, mozna ze ty posledni measures taky
scidb trochu rozsekcovat je to moc velkej blok textu

multidim. appr. pat. match. znova pripomenout o co tam jde, moc hodne textu trochu rozsekat

treba neco o kernelovych metodach na polich

amr zaradit pod quadstromy a napsat jak se to k nim poji
mat je spis indexovaci metoda a mozna zkratit

mapreduce dat taky pod neco

marginals taky zanorit do neco
pridat treba k agregacnim problemum od soroushe

nejdriv multidimensionalni pole a pak teprve o Scidb, po SciDB usecase ze SciDB

v implementaci:
konkretnejsi, rozepsat 
o generovani napsat nejaky pravdepodobnosti, nakreslit obrazky treba (asi jak vypadaj ty gausovky), jeste to vic rozepsat
u algoritmu rozepsat vic do detailu, pridat slozitosti 

output presunout za input, nebo na konec
dat priklad input dat, 
u chunkovani priklady, obrazky, 
neni potreba v kazdym odstavci rikat ze je na to pythoni script, misto toho dat na zacatek implementace seznam vsech toolu a ze jsou v pythonu
nebude v tech castech ani jednou slovo python
nepsat ze to umi jen s intama a vsechno ostatni jsou stringy, misto toho napsat ze to umi s floatama (protoze ty to pretypuje na int a bude v pohode)
popisy parametru nejsou podstatny
popsat implementacni challenge - ze lokalita je zajistena chunkama a tak
odvodit jednoduchou pametovou slozitost typu, v jednu chvili jedno vlakne nepotrebuje vic nez blabla*chunku mista

obrazky k hashed stricter filteru treba jak vypadaj ty kernely, je to moc velky blok textu, udelat treba odstavce
ta assumption jak to zavisi na cem, kolik bude hashu a blablabla

vsechno vysvetlovat hodne na prikladech, v prvni sekci vysvetlit vsechno teoreticky s priklady jak to je a v druhy konkretne implementaci

klidne v ty teoreticky sekci to vzit jako navaruv clanek a prevysvetlit to svyma slovama, bude to delsi, ale to neva a s prikladama

to s hashovanou preverifikaci vic vysvetlit, lepe vysvetlit proc to neni vyhodny, kdybych to ja zahashovala jednou predem, tak mam super index ktery by to hodne urychlil
proc teda nemit jeden malej index u toho pole, lip to rozvist, duvod ze jsme se otlik nezamerovali na indexy ale spis na matchovani

misto exact a approximate bude jedna kapitola, python vedle, pak sekce dalsi napady a blabal

4GB je malo sloupcu, ty patterny do toho nemichat, chtelo by to 1GB file mezi tim

projet chybu od 0 do 128, pro ten standard file

v ty velikosti dat slo o to aby to zevnitr melo 256 MB, protoze to ze tam nemam dimenze muze byt i u vstupniho filu, takze to je spatne a je potreba to bud predelat, 
nebo si to obhajit nejak, ten dense neskaluje s dimenzionalitou stejne jako ty chunky
o tech 2 bytech v binarnim formatu nepsat, tenhle format musi byt stejny vsude
vysvetlit proc je ta maska tak velka, rozepsat

u tech realnych datasetu, nepsat jako duvod ze jsou moc maly

to dynamicky programovani z toho vytahnout

pak o kazdy kapitole rict, ze to je muj prinos a nikdo to jeste neudelal

